{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_bert.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPznu0k8acAgUS5gcXaSdP7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fawazshah/News-Media-Reliability/blob/master/train_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v3OZRfOXciF"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WdkCXGSlBCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b776ef58-0b38-4c63-a64c-02d507bb3c87"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\r\u001b[K     |▏                               | 10kB 25.8MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 32.0MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 21.9MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 25.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 24.7MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 27.3MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 22.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 81kB 24.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 92kB 21.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 102kB 22.0MB/s eta 0:00:01\r\u001b[K     |██                              | 112kB 22.0MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 22.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 133kB 22.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 143kB 22.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 153kB 22.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 163kB 22.0MB/s eta 0:00:01\r\u001b[K     |███                             | 174kB 22.0MB/s eta 0:00:01\r\u001b[K     |███                             | 184kB 22.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 194kB 22.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 204kB 22.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 215kB 22.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 225kB 22.0MB/s eta 0:00:01\r\u001b[K     |████                            | 235kB 22.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 245kB 22.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 256kB 22.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 266kB 22.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 276kB 22.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 286kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 296kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 307kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 317kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 327kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 337kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 348kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 358kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 368kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 378kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 389kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 399kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 409kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 419kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 430kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 440kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 450kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 460kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 471kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 481kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 491kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 501kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 512kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 522kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 532kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 542kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 552kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 563kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 573kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 583kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 593kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 604kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 614kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 624kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 634kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 645kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 655kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 665kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 675kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 686kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 696kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 706kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 716kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 727kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 737kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 747kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 757kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 768kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 778kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 788kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 798kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 808kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 819kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 829kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 839kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 849kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 860kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 870kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 880kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 890kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 901kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 911kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 921kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 931kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 942kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 952kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 962kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 972kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 983kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 993kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.0MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.0MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.0MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.0MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.0MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.1MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.1MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.1MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.1MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.1MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.1MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.1MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.2MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.2MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.2MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.2MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.2MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.2MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.2MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.3MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.3MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.3MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.3MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.3MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.3MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.3MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.3MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.3MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.4MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.4MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.4MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.4MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.4MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.4MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.4MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.4MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.4MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.4MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.5MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.5MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.5MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.5MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.5MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.5MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.5MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.5MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.6MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.6MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.6MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.6MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.6MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.6MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.6MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.6MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.6MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.6MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.7MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.7MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.7MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.7MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.7MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.7MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.7MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.7MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.7MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.8MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.8MB 22.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.8MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.8MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.8MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.8MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.8MB 22.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.8MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.8MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.8MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.9MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.9MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.9MB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.9MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.9MB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.9MB 22.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 55.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=d7e7e471ff33757d30c1f91e90512483808b2edbcf5d57e91baac921508191bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH89boyR8xMe"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import requests\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "import time\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvijqpDgSdrG"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acNz10pHhN2o"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOjS9Y-7hJG6"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "class HiddenPrints:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout = self._original_stdout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdQJh_Gspywv"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ltm1AzQ5P2p"
      },
      "source": [
        "corpus_url = 'https://raw.githubusercontent.com/fawazshah/News-Media-Reliability/master/data/emnlp18/corpus-modified.tsv'\n",
        "\n",
        "corpus = pd.read_csv(corpus_url, sep='\\t')\n",
        "urls = corpus['source_url_normalized'].values\n",
        "\n",
        "# Ground truths\n",
        "biases = corpus['bias'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aljGD_19QdV"
      },
      "source": [
        "article_data_json_url = 'https://raw.githubusercontent.com/fawazshah/News-Media-Reliability/master/data/scraped_articles.json'\n",
        "\n",
        "r = requests.get(article_data_json_url)\n",
        "article_data = r.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcqkrRXyw58K"
      },
      "source": [
        "all_data_df = pd.DataFrame(columns=['article headline', 'article body', 'bias'])\n",
        "\n",
        "news_sources_scraped = 0\n",
        "\n",
        "for row in corpus.itertuples():\n",
        "    url = row.source_url_normalized\n",
        "    bias = row.bias\n",
        "    if article_data[\"newspapers\"][url] is not None:\n",
        "        articles = article_data[\"newspapers\"][url].get(\"articles\", [])\n",
        "        if len(articles) > 0:\n",
        "            news_sources_scraped += 1\n",
        "            for article in articles:\n",
        "                all_data_df = all_data_df.append({'article headline': article['title'],\n",
        "                                                  'article body': article['text'],\n",
        "                                                  'bias': bias}, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "o8iAM1Mb1-Mr",
        "outputId": "8caaf71b-a8c9-49c5-af62-eb58969988d2"
      },
      "source": [
        "all_data_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article headline</th>\n",
              "      <th>article body</th>\n",
              "      <th>bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>On the Ground at the Inauguration: The Only Th...</td>\n",
              "      <td>Will Sennott\\n\\nWEDNESDAY, JANUARY 20, 2021, W...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Under President Biden, Will the Yankees Return...</td>\n",
              "      <td>Thurman Munson and Reggie Jackson in 1977 From...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Gun Rights Absolutists Celebrate Martin Luther...</td>\n",
              "      <td>Will Sennott\\n\\nMONDAY, JANUARY 18, 2021, RICH...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thugs in Blue</td>\n",
              "      <td>THE BEAT GOES ON … AND ON\\n\\nOnce Again, Polic...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HELL YEAH! Sheriff Clark Publicly DISEMBOWELS ...</td>\n",
              "      <td>Al Sharpton always has had a couple screws loo...</td>\n",
              "      <td>right</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1649</th>\n",
              "      <td>UK Educators Rank-and-File Safety Committee di...</td>\n",
              "      <td>The UK Educators Rank-and-File Safety Committe...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1650</th>\n",
              "      <td>Make It Sing</td>\n",
              "      <td>Before I lay into the Democrats for missed opp...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1651</th>\n",
              "      <td>Bill Maher: The SPIN Interview</td>\n",
              "      <td>If you care at all about democracy and the way...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>Stephan Jenkins on What Culture Truly Means</td>\n",
              "      <td>“When bad men combine, the good must associate...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1653</th>\n",
              "      <td>Emergence Is Interactive: A Jeff Bridges and S...</td>\n",
              "      <td>Jeff Bridges is an Academy Award-winning actor...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1654 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       article headline  ...   bias\n",
              "0     On the Ground at the Inauguration: The Only Th...  ...   left\n",
              "1     Under President Biden, Will the Yankees Return...  ...   left\n",
              "2     Gun Rights Absolutists Celebrate Martin Luther...  ...   left\n",
              "3                                         Thugs in Blue  ...   left\n",
              "4     HELL YEAH! Sheriff Clark Publicly DISEMBOWELS ...  ...  right\n",
              "...                                                 ...  ...    ...\n",
              "1649  UK Educators Rank-and-File Safety Committee di...  ...   left\n",
              "1650                                       Make It Sing  ...   left\n",
              "1651                     Bill Maher: The SPIN Interview  ...   left\n",
              "1652        Stephan Jenkins on What Culture Truly Means  ...   left\n",
              "1653  Emergence Is Interactive: A Jeff Bridges and S...  ...   left\n",
              "\n",
              "[1654 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3tI0VYup1RN"
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx8wTRfPI5jm",
        "outputId": "e9e52689-2484-4a50-d134-d415461c7af1"
      },
      "source": [
        "# Text preprocessing preparation\n",
        "\n",
        "stop_words = [\"the\", \"a\", \"an\", \"as\", \"this\", \"that\", \"is\", \"and\", \"or\", \"on\",\n",
        "              \"at\", \"to\", \"in\", \"by\", \"than\", \"of\", \"for\", \"be\", \"i\", \"you\", \n",
        "              \"he\", \"she\", \"his\", \"her\", \"do\", \"it\", \"with\"]\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# required for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# required for POS tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_vUJWRoo8uy"
      },
      "source": [
        "# Text preprocessing performed on both article headline and article body\n",
        "\n",
        "def preprocess(sentence):\n",
        "\n",
        "    # Lowercase\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Punctuation, whitespace removal\n",
        "    punctuations = '''!()-—[]{};:'\"“”‘’\\,<>./?@#$%^&*_~'''\n",
        "    whitespace = '''\\n\\t'''\n",
        "\n",
        "    for ch in sentence: \n",
        "        if ch in punctuations: \n",
        "            sentence = sentence.replace(ch, \"\")\n",
        "        if ch in whitespace:\n",
        "            sentence = sentence.replace(ch, \" \")\n",
        "\n",
        "    # Stop word removal\n",
        "    remaining_words = []\n",
        "    \n",
        "    for word in sentence.split():\n",
        "        if word not in stop_words:\n",
        "            remaining_words.append(word)\n",
        "\n",
        "    sentence = \" \".join(remaining_words)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_words = []\n",
        "\n",
        "    # In order to lemmatise we must first POS-tag each sentence\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "    for word, tag in tagged:\n",
        "        pos = nltk_tag_to_wordnet_tag(tag) \n",
        "        if pos is not None:\n",
        "            word = lemmatizer.lemmatize(word, pos=pos)\n",
        "\n",
        "        lemmatized_words.append(word)\n",
        "\n",
        "    sentence = \" \".join(lemmatized_words)\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3U6E9s6rRbxK",
        "outputId": "0ea59171-f6b2-4702-cfb5-403df0e76e34"
      },
      "source": [
        "start = time.time()\n",
        "all_data_df['article headline'] = all_data_df['article headline'].apply(preprocess)\n",
        "print(f\"Preprocessing headlines took {time.time() - start} seconds\")\n",
        "\n",
        "start = time.time()\n",
        "all_data_df['article body'] = all_data_df['article body'].apply(preprocess)\n",
        "print(f\"Preprocessing article bodies took {time.time() - start} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing headlines took 1.0587806701660156 seconds\n",
            "Preprocessing article bodies took 47.80686569213867 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q95veuSydbIW"
      },
      "source": [
        "# Encode labels as numbers\n",
        "# center == 0\n",
        "# left == 1\n",
        "# right == 2\n",
        "\n",
        "def encode_labels(label):\n",
        "    if label == \"center\":\n",
        "        return 0\n",
        "    elif label == \"left\":\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "all_data_df['bias'] = all_data_df['bias'].apply(encode_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGgA4zrUes86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50508583-11a4-43e2-d0ef-b209a63330a4"
      },
      "source": [
        "all_data_df['bias'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    657\n",
              "0    567\n",
              "1    430\n",
              "Name: bias, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "nlw6MBB_lsoV",
        "outputId": "1b8e3c27-e3aa-4e71-a1de-5a8fd39a9705"
      },
      "source": [
        "# Randomly shuffle rows in dataset before splitting into folds\n",
        "all_data_df = all_data_df.sample(frac=1, random_state=1)\n",
        "all_data_df.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article headline</th>\n",
              "      <th>article body</th>\n",
              "      <th>bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bidens america one nation us versus them</td>\n",
              "      <td>president joe biden sworn 46th president janua...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>how get covid19 vaccine miamidade broward</td>\n",
              "      <td>keep new time free support us local community ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>arm mob storm capitol building during electora...</td>\n",
              "      <td>day will go down infamy arm mob storm united s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>frontier ebook release january 2021</td>\n",
              "      <td>download month new release include late specia...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>change date vaccine news angry cricket coach</td>\n",
              "      <td>clancy overell wendell hussey kick off another...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1649</th>\n",
              "      <td>legal liability loom orgs behind rally incite ...</td>\n",
              "      <td>legal liability loom orgs behind rally incite ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1650</th>\n",
              "      <td>merck france pasteur institute end development...</td>\n",
              "      <td>covid19 pandemic underscore need our company o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1651</th>\n",
              "      <td>anthony mackie responsibility message captain ...</td>\n",
              "      <td>anthony mackie clear not all say he new captai...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>union just get rare bit good news from supreme...</td>\n",
              "      <td>supreme court announce monday will not hear bl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1653</th>\n",
              "      <td>florida new hq maga movement</td>\n",
              "      <td>former president donald trump remain iffy 2024...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1654 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       article headline  ... bias\n",
              "0              bidens america one nation us versus them  ...    2\n",
              "1             how get covid19 vaccine miamidade broward  ...    1\n",
              "2     arm mob storm capitol building during electora...  ...    1\n",
              "3                   frontier ebook release january 2021  ...    0\n",
              "4          change date vaccine news angry cricket coach  ...    0\n",
              "...                                                 ...  ...  ...\n",
              "1649  legal liability loom orgs behind rally incite ...  ...    1\n",
              "1650  merck france pasteur institute end development...  ...    0\n",
              "1651  anthony mackie responsibility message captain ...  ...    1\n",
              "1652  union just get rare bit good news from supreme...  ...    1\n",
              "1653                       florida new hq maga movement  ...    2\n",
              "\n",
              "[1654 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CtZd7-n1tJ_"
      },
      "source": [
        "### Randomly selecting body text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4lljJnu1wwZ"
      },
      "source": [
        "def get_random_512(sentence):\n",
        "    toks = sentence.split()\n",
        "    max_start = len(toks) - 512 - 1 if len(toks) > 512 else 0\n",
        "    start_index = random.randint(0, max_start)\n",
        "    return ' '.join(toks[start_index:start_index+512])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMo_ORFzaw3W"
      },
      "source": [
        "RANDOM_SELECT = False\n",
        "\n",
        "if RANDOM_SELECT:\n",
        "    all_data_df['article body'] = all_data_df['article body'].apply(get_random_512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCS4LqNqpw-0"
      },
      "source": [
        "### Splitting data into folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WVsMZJip6pD"
      },
      "source": [
        "# 5 folds, each with 70% training, 10% validation, 20% train\n",
        "\n",
        "num_folds = 5\n",
        "\n",
        "fold_size = round(len(all_data_df) / num_folds)\n",
        "fold_dfs = [all_data_df.iloc[i*fold_size:(i+1)*fold_size].copy() for i in range(num_folds)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcLgT3JPrI5l"
      },
      "source": [
        "folds = {}\n",
        "\n",
        "for i, df in enumerate(fold_dfs):\n",
        "    folds[i] = {}\n",
        "    split_point_1 = int(0.7*len(df))\n",
        "    split_point_2 = int(0.8*len(df))\n",
        "    folds[i][\"train_df\"] = df.iloc[:split_point_1].copy()\n",
        "    folds[i][\"val_df\"] = df.iloc[split_point_1:split_point_2].copy()\n",
        "    folds[i][\"test_df\"] = df.iloc[split_point_2:].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv18NbamsoT8",
        "outputId": "a6de0f67-a9a6-4357-d2e7-50c1989bfb7c"
      },
      "source": [
        "print(f\"Number of folds: {num_folds}\")\n",
        "print(f\"Size of each training set: {len(folds[0]['train_df'])}\")\n",
        "print(f\"Size of each validation set: {len(folds[0]['val_df'])}\")\n",
        "print(f\"Size of each test set: {len(folds[0]['test_df'])}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of folds: 5\n",
            "Size of each training set: 231\n",
            "Size of each validation set: 33\n",
            "Size of each test set: 67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRvLmb5gjsqW"
      },
      "source": [
        "### BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-qdjK0sTCvE"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noOaI7TwFfO2"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdMOKYheI2rI"
      },
      "source": [
        "# Compute the length of the longest sentence in particular column out of\n",
        "# all train, val and test data\n",
        "def compute_max_length(col_to_encode):\n",
        "\n",
        "  sentences = all_data_df[col_to_encode].values\n",
        "\n",
        "  max_len = 0\n",
        "\n",
        "  for sent in sentences:\n",
        "\n",
        "      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "      input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "      # Update the maximum sentence length.\n",
        "      max_len = max(max_len, len(input_ids))\n",
        "\n",
        "  return max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLUvQg9tJkxL",
        "outputId": "29c570da-a31e-4516-a5da-3adeafe7de14"
      },
      "source": [
        "max_len_headline = compute_max_length('article headline')\n",
        "max_len_body = compute_max_length('article body')\n",
        "\n",
        "print(f\"Max headline length across all folds: {max_len_headline}\")\n",
        "print(f\"Max article body length across all folds: {max_len_body}\")\n",
        "\n",
        "if max_len_headline > 512:\n",
        "    max_len_headline = 512\n",
        "if max_len_body > 512:\n",
        "    max_len_body = 512"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Max headline length across all folds: 103\n",
            "Max article body length across all folds: 3049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47kpzdncrYs_"
      },
      "source": [
        "def create_bert_dataset(df, col_to_encode, max_sequence_len):\n",
        "    # Returns a TensorDataset of sequences in col_to_encode column of df\n",
        "\n",
        "    token_ids = []\n",
        "    token_type_ids = [] # segment ids \n",
        "    attention_masks = []\n",
        "\n",
        "    sentences = df[col_to_encode].values.tolist()\n",
        "    for sent in sentences:\n",
        "        encoding_dict = tokenizer(sent,\n",
        "                                  add_special_tokens=True,\n",
        "                                  max_length=max_sequence_len,\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  return_token_type_ids = True,\n",
        "                                  return_attention_mask = True,\n",
        "                                  return_tensors = 'pt'\n",
        "                                  )\n",
        "        token_ids.append(encoding_dict['input_ids'])\n",
        "        token_type_ids.append(encoding_dict['token_type_ids'])\n",
        "        attention_masks.append(encoding_dict['attention_mask'])\n",
        "    \n",
        "    token_ids = torch.cat(token_ids, dim=0)\n",
        "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(df['bias'].values)\n",
        "    \n",
        "    return TensorDataset(token_ids, token_type_ids, attention_masks, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xefnAsZO59ZV"
      },
      "source": [
        "dataloaders = {}\n",
        "batch_size = 10\n",
        "\n",
        "# Note we manually shuffled the dataset earlier, so we can use SequentialSampler to\n",
        "# sample instead of RandomSampler during training\n",
        "\n",
        "dataloaders['headlines'] = {}\n",
        "for i in range(num_folds):\n",
        "    dataloaders['headlines'][i] = {}\n",
        "    train_dataset = create_bert_dataset(folds[i]['train_df'], 'article headline', max_len_headline)\n",
        "    dataloaders['headlines'][i]['train'] = DataLoader(train_dataset, sampler=SequentialSampler(train_dataset), batch_size=batch_size)\n",
        "    val_dataset = create_bert_dataset(folds[i]['val_df'], 'article headline', max_len_headline)\n",
        "    dataloaders['headlines'][i]['val'] = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
        "    test_dataset = create_bert_dataset(folds[i]['test_df'], 'article headline', max_len_headline)\n",
        "    dataloaders['headlines'][i]['test'] = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
        "\n",
        "dataloaders['bodies'] = {}\n",
        "for i in range(num_folds):\n",
        "    dataloaders['bodies'][i] = {}\n",
        "    train_dataset = create_bert_dataset(folds[i]['train_df'], 'article body', max_len_body)\n",
        "    dataloaders['bodies'][i]['train'] = DataLoader(train_dataset, sampler=SequentialSampler(train_dataset), batch_size=batch_size)\n",
        "    val_dataset = create_bert_dataset(folds[i]['val_df'], 'article body', max_len_body)\n",
        "    dataloaders['bodies'][i]['val'] = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
        "    test_dataset = create_bert_dataset(folds[i]['test_df'], 'article body', max_len_body)\n",
        "    dataloaders['bodies'][i]['test'] = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uysrdNBJx0G"
      },
      "source": [
        "def train_BERT(train_dataloader, val_dataloader, model, number_epoch):\n",
        "\n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, \n",
        "                    eps = 1e-8 \n",
        "                )\n",
        "\n",
        "    # Create the learning rate scheduler.\n",
        "    total_steps = len(train_dataloader) * number_epoch\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, \n",
        "                                                num_training_steps=total_steps)\n",
        "\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "\n",
        "        # TRAINING\n",
        "\n",
        "        time0 = time.time()\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        epoch_train_loss = 0\n",
        "        no_observations = 0\n",
        "        epoch_train_predictions = []\n",
        "        epoch_train_labels = []\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "\n",
        "            # Each batch contains token ids, token type ids, attention masks and labels\n",
        "            b_token_ids = batch[0].to(device)\n",
        "            b_token_type_ids = batch[1].to(device)\n",
        "            b_attention_masks = batch[2].to(device)\n",
        "            b_labels = batch[3].to(device)\n",
        "\n",
        "            no_observations = no_observations + b_labels.shape[0]\n",
        "            \n",
        "            output = model(b_token_ids, \n",
        "                    token_type_ids=b_token_type_ids, \n",
        "                    attention_mask=b_attention_masks, \n",
        "                    labels=b_labels)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            loss = output.loss\n",
        "            logits = output.logits\n",
        "\n",
        "            predictions = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "            labels = b_labels.detach().cpu().numpy()\n",
        "            epoch_train_predictions.extend(predictions)\n",
        "            epoch_train_labels.extend(labels)\n",
        "\n",
        "            loss.backward()\n",
        "            # Clip the norm of the gradients to 1 to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step() \n",
        "\n",
        "            # Update the learning rate using the scheduler\n",
        "            scheduler.step()  \n",
        "\n",
        "            epoch_train_loss += loss.item()*b_labels.shape[0]\n",
        "\n",
        "        epoch_train_loss, epoch_train_acc = epoch_train_loss / no_observations, accuracy_score(epoch_train_labels, epoch_train_predictions)\n",
        "\n",
        "        # VALIDATION\n",
        "\n",
        "        epoch_valid_loss, epoch_val_predictions, epoch_val_labels = evaluate_BERT(val_dataloader, model)\n",
        "        epoch_valid_acc = accuracy_score(epoch_val_labels, epoch_val_predictions)\n",
        "\n",
        "        # FINALLY\n",
        "\n",
        "        print(f\"Epoch took: {time.time() - time0}\")\n",
        "\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_train_loss:.2f} | Train Accuracy: {epoch_train_acc:.2f} | \\\n",
        "        Val. Loss: {epoch_valid_loss:.2f} | Val. Accuracy: {epoch_valid_acc:.2f} |')\n",
        "\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        valid_loss.append(epoch_valid_loss)\n",
        "    \n",
        "    return train_loss, valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxb6dGkmRLky"
      },
      "source": [
        "def evaluate_BERT(test_dataloader, model):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    no_observations = 0\n",
        "    predictions_all = []\n",
        "    labels_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            b_token_ids = batch[0].to(device)\n",
        "            b_token_type_ids = batch[1].to(device)\n",
        "            b_attention_masks = batch[2].to(device)\n",
        "            b_labels = batch[3].to(device)\n",
        "\n",
        "            no_observations += b_labels.shape[0]\n",
        "            output = model(b_token_ids, token_type_ids=b_token_type_ids, \n",
        "                                        attention_mask=b_attention_masks)\n",
        "            logits = output.logits\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "\n",
        "            predictions = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "            labels = b_labels.detach().cpu().numpy()\n",
        "            predictions_all.extend(predictions)\n",
        "            labels_all.extend(labels)\n",
        "\n",
        "            total_loss += loss.item()*b_labels.shape[0]\n",
        "    \n",
        "    return total_loss / no_observations, predictions_all, labels_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-AFqf9cQIfz",
        "outputId": "23b178ff-2eca-426e-8954-bc040ba7796a"
      },
      "source": [
        "num_epochs = 4\n",
        "\n",
        "data_types = ['headlines', 'bodies']\n",
        "\n",
        "for data_type in data_types:\n",
        "\n",
        "    f1_scores = []\n",
        "    accuracies = []\n",
        "\n",
        "    for i in range(num_folds):\n",
        "\n",
        "        print(f\"-------------------\")\n",
        "        print(f\"{data_type.upper()} - FOLD {i}\")\n",
        "        print(f\"-------------------\")\n",
        "\n",
        "        # Set up a new BERT model\n",
        "        model = BertForSequenceClassification.from_pretrained(\n",
        "            'bert-base-uncased',\n",
        "            num_labels=3,\n",
        "            output_attentions = False,\n",
        "            output_hidden_states = False,\n",
        "        )\n",
        "        model.cuda()\n",
        "\n",
        "        # Train model\n",
        "        train_dataloader = dataloaders[data_type][i]['train']\n",
        "        val_dataloader = dataloaders[data_type][i]['val']\n",
        "        train_loss, valid_loss = train_BERT(train_dataloader, val_dataloader, model, num_epochs)\n",
        "\n",
        "        # Test model\n",
        "        test_dataloader = dataloaders[data_type][i]['test']\n",
        "        _, predictions, labels = evaluate_BERT(test_dataloader, model)\n",
        "\n",
        "        f1_scores.append(f1_score(labels, predictions, average='macro'))\n",
        "        accuracies.append(accuracy_score(labels, predictions))\n",
        "        \n",
        "        print(classification_report(labels, predictions))\n",
        "    \n",
        "    print(f\"---------------------------------------\")\n",
        "    print(f\"{data_type.upper()} - FINAL TEST RESULTS\")\n",
        "    print(f\"---------------------------------------\")\n",
        "\n",
        "    print(f\"Average F1: {sum(f1_scores) / len(f1_scores)}\")\n",
        "    print(f\"Average accuracy: {sum(accuracies) / len(accuracies)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "HEADLINES - FOLD 0\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch took: 5.3621826171875\n",
            "| Epoch: 01 | Train Loss: 1.10 | Train Accuracy: 0.32 |         Val. Loss: 1.07 | Val. Accuracy: 0.30 |\n",
            "Epoch took: 5.416951417922974\n",
            "| Epoch: 02 | Train Loss: 1.05 | Train Accuracy: 0.42 |         Val. Loss: 1.05 | Val. Accuracy: 0.36 |\n",
            "Epoch took: 5.485903978347778\n",
            "| Epoch: 03 | Train Loss: 0.97 | Train Accuracy: 0.55 |         Val. Loss: 1.01 | Val. Accuracy: 0.45 |\n",
            "Epoch took: 5.474928140640259\n",
            "| Epoch: 04 | Train Loss: 0.90 | Train Accuracy: 0.66 |         Val. Loss: 1.00 | Val. Accuracy: 0.45 |\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.12      0.18        24\n",
            "           1       0.10      0.06      0.08        16\n",
            "           2       0.46      0.81      0.59        27\n",
            "\n",
            "    accuracy                           0.39        67\n",
            "   macro avg       0.30      0.33      0.28        67\n",
            "weighted avg       0.33      0.39      0.32        67\n",
            "\n",
            "-------------------\n",
            "HEADLINES - FOLD 1\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch took: 5.567697763442993\n",
            "| Epoch: 01 | Train Loss: 1.08 | Train Accuracy: 0.39 |         Val. Loss: 1.10 | Val. Accuracy: 0.36 |\n",
            "Epoch took: 5.595129013061523\n",
            "| Epoch: 02 | Train Loss: 0.97 | Train Accuracy: 0.61 |         Val. Loss: 1.10 | Val. Accuracy: 0.39 |\n",
            "Epoch took: 5.6418163776397705\n",
            "| Epoch: 03 | Train Loss: 0.84 | Train Accuracy: 0.71 |         Val. Loss: 1.17 | Val. Accuracy: 0.27 |\n",
            "Epoch took: 5.612733602523804\n",
            "| Epoch: 04 | Train Loss: 0.79 | Train Accuracy: 0.73 |         Val. Loss: 1.15 | Val. Accuracy: 0.30 |\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.71      0.58        24\n",
            "           1       1.00      0.18      0.30        17\n",
            "           2       0.48      0.54      0.51        26\n",
            "\n",
            "    accuracy                           0.51        67\n",
            "   macro avg       0.66      0.47      0.46        67\n",
            "weighted avg       0.62      0.51      0.48        67\n",
            "\n",
            "-------------------\n",
            "HEADLINES - FOLD 2\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch took: 5.589334011077881\n",
            "| Epoch: 01 | Train Loss: 1.11 | Train Accuracy: 0.33 |         Val. Loss: 1.08 | Val. Accuracy: 0.39 |\n",
            "Epoch took: 5.467073202133179\n",
            "| Epoch: 02 | Train Loss: 1.03 | Train Accuracy: 0.54 |         Val. Loss: 1.07 | Val. Accuracy: 0.55 |\n",
            "Epoch took: 5.4414732456207275\n",
            "| Epoch: 03 | Train Loss: 0.94 | Train Accuracy: 0.70 |         Val. Loss: 1.05 | Val. Accuracy: 0.55 |\n",
            "Epoch took: 5.438376426696777\n",
            "| Epoch: 04 | Train Loss: 0.87 | Train Accuracy: 0.69 |         Val. Loss: 1.05 | Val. Accuracy: 0.55 |\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.60      0.53        20\n",
            "           1       0.27      0.14      0.18        22\n",
            "           2       0.42      0.52      0.46        25\n",
            "\n",
            "    accuracy                           0.42        67\n",
            "   macro avg       0.39      0.42      0.39        67\n",
            "weighted avg       0.39      0.42      0.39        67\n",
            "\n",
            "-------------------\n",
            "HEADLINES - FOLD 3\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch took: 5.426418304443359\n",
            "| Epoch: 01 | Train Loss: 1.09 | Train Accuracy: 0.39 |         Val. Loss: 1.05 | Val. Accuracy: 0.52 |\n",
            "Epoch took: 5.404155492782593\n",
            "| Epoch: 02 | Train Loss: 1.04 | Train Accuracy: 0.45 |         Val. Loss: 1.03 | Val. Accuracy: 0.55 |\n",
            "Epoch took: 5.420147657394409\n",
            "| Epoch: 03 | Train Loss: 0.96 | Train Accuracy: 0.58 |         Val. Loss: 0.99 | Val. Accuracy: 0.52 |\n",
            "Epoch took: 5.46432089805603\n",
            "| Epoch: 04 | Train Loss: 0.88 | Train Accuracy: 0.68 |         Val. Loss: 0.98 | Val. Accuracy: 0.52 |\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.52      0.50        25\n",
            "           1       0.20      0.06      0.09        17\n",
            "           2       0.49      0.68      0.57        25\n",
            "\n",
            "    accuracy                           0.46        67\n",
            "   macro avg       0.39      0.42      0.39        67\n",
            "weighted avg       0.41      0.46      0.42        67\n",
            "\n",
            "-------------------\n",
            "HEADLINES - FOLD 4\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch took: 5.463284492492676\n",
            "| Epoch: 01 | Train Loss: 1.09 | Train Accuracy: 0.37 |         Val. Loss: 1.07 | Val. Accuracy: 0.38 |\n",
            "Epoch took: 5.413251161575317\n",
            "| Epoch: 02 | Train Loss: 1.04 | Train Accuracy: 0.47 |         Val. Loss: 1.07 | Val. Accuracy: 0.50 |\n",
            "Epoch took: 5.431013584136963\n",
            "| Epoch: 03 | Train Loss: 0.95 | Train Accuracy: 0.60 |         Val. Loss: 1.04 | Val. Accuracy: 0.47 |\n",
            "Epoch took: 5.435129642486572\n",
            "| Epoch: 04 | Train Loss: 0.90 | Train Accuracy: 0.63 |         Val. Loss: 1.03 | Val. Accuracy: 0.50 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.33      0.46        33\n",
            "           1       0.00      0.00      0.00        13\n",
            "           2       0.33      0.85      0.48        20\n",
            "\n",
            "    accuracy                           0.42        66\n",
            "   macro avg       0.36      0.39      0.31        66\n",
            "weighted avg       0.47      0.42      0.37        66\n",
            "\n",
            "-------------------\n",
            "BODIES - FOLD 0\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch took: 26.343168258666992\n",
            "| Epoch: 01 | Train Loss: 1.10 | Train Accuracy: 0.34 |         Val. Loss: 1.02 | Val. Accuracy: 0.58 |\n",
            "Epoch took: 26.184012413024902\n",
            "| Epoch: 02 | Train Loss: 1.00 | Train Accuracy: 0.55 |         Val. Loss: 0.95 | Val. Accuracy: 0.64 |\n",
            "Epoch took: 26.1992244720459\n",
            "| Epoch: 03 | Train Loss: 0.88 | Train Accuracy: 0.61 |         Val. Loss: 0.89 | Val. Accuracy: 0.61 |\n",
            "Epoch took: 26.349560499191284\n",
            "| Epoch: 04 | Train Loss: 0.80 | Train Accuracy: 0.66 |         Val. Loss: 0.87 | Val. Accuracy: 0.61 |\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.62      0.57        24\n",
            "           1       0.00      0.00      0.00        16\n",
            "           2       0.53      0.74      0.62        27\n",
            "\n",
            "    accuracy                           0.52        67\n",
            "   macro avg       0.35      0.46      0.39        67\n",
            "weighted avg       0.40      0.52      0.45        67\n",
            "\n",
            "-------------------\n",
            "BODIES - FOLD 1\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch took: 26.420538425445557\n",
            "| Epoch: 01 | Train Loss: 1.10 | Train Accuracy: 0.41 |         Val. Loss: 1.08 | Val. Accuracy: 0.42 |\n",
            "Epoch took: 26.430352210998535\n",
            "| Epoch: 02 | Train Loss: 1.01 | Train Accuracy: 0.55 |         Val. Loss: 1.06 | Val. Accuracy: 0.48 |\n",
            "Epoch took: 26.208841800689697\n",
            "| Epoch: 03 | Train Loss: 0.89 | Train Accuracy: 0.64 |         Val. Loss: 1.06 | Val. Accuracy: 0.52 |\n",
            "Epoch took: 26.39699935913086\n",
            "| Epoch: 04 | Train Loss: 0.79 | Train Accuracy: 0.68 |         Val. Loss: 1.08 | Val. Accuracy: 0.48 |\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.54      0.51        24\n",
            "           1       0.00      0.00      0.00        17\n",
            "           2       0.40      0.62      0.48        26\n",
            "\n",
            "    accuracy                           0.43        67\n",
            "   macro avg       0.29      0.39      0.33        67\n",
            "weighted avg       0.33      0.43      0.37        67\n",
            "\n",
            "-------------------\n",
            "BODIES - FOLD 2\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch took: 26.49622082710266\n",
            "| Epoch: 01 | Train Loss: 1.09 | Train Accuracy: 0.38 |         Val. Loss: 1.09 | Val. Accuracy: 0.42 |\n",
            "Epoch took: 26.45432996749878\n",
            "| Epoch: 02 | Train Loss: 1.09 | Train Accuracy: 0.37 |         Val. Loss: 1.07 | Val. Accuracy: 0.45 |\n",
            "Epoch took: 26.24441623687744\n",
            "| Epoch: 03 | Train Loss: 1.06 | Train Accuracy: 0.43 |         Val. Loss: 1.06 | Val. Accuracy: 0.48 |\n",
            "Epoch took: 26.500881671905518\n",
            "| Epoch: 04 | Train Loss: 1.00 | Train Accuracy: 0.53 |         Val. Loss: 1.06 | Val. Accuracy: 0.48 |\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.30      0.33        20\n",
            "           1       0.00      0.00      0.00        22\n",
            "           2       0.39      0.80      0.53        25\n",
            "\n",
            "    accuracy                           0.39        67\n",
            "   macro avg       0.26      0.37      0.29        67\n",
            "weighted avg       0.26      0.39      0.30        67\n",
            "\n",
            "-------------------\n",
            "BODIES - FOLD 3\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch took: 26.266059637069702\n",
            "| Epoch: 01 | Train Loss: 1.11 | Train Accuracy: 0.35 |         Val. Loss: 1.05 | Val. Accuracy: 0.58 |\n",
            "Epoch took: 26.288325548171997\n",
            "| Epoch: 02 | Train Loss: 1.07 | Train Accuracy: 0.42 |         Val. Loss: 1.03 | Val. Accuracy: 0.58 |\n",
            "Epoch took: 26.162044525146484\n",
            "| Epoch: 03 | Train Loss: 1.03 | Train Accuracy: 0.48 |         Val. Loss: 1.03 | Val. Accuracy: 0.55 |\n",
            "Epoch took: 26.10634994506836\n",
            "| Epoch: 04 | Train Loss: 0.98 | Train Accuracy: 0.56 |         Val. Loss: 1.01 | Val. Accuracy: 0.52 |\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.52      0.58        25\n",
            "           1       0.00      0.00      0.00        17\n",
            "           2       0.47      0.88      0.61        25\n",
            "\n",
            "    accuracy                           0.52        67\n",
            "   macro avg       0.37      0.47      0.40        67\n",
            "weighted avg       0.42      0.52      0.44        67\n",
            "\n",
            "-------------------\n",
            "BODIES - FOLD 4\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch took: 26.31061029434204\n",
            "| Epoch: 01 | Train Loss: 1.09 | Train Accuracy: 0.41 |         Val. Loss: 1.09 | Val. Accuracy: 0.38 |\n",
            "Epoch took: 26.349208116531372\n",
            "| Epoch: 02 | Train Loss: 1.06 | Train Accuracy: 0.43 |         Val. Loss: 1.09 | Val. Accuracy: 0.47 |\n",
            "Epoch took: 26.075583696365356\n",
            "| Epoch: 03 | Train Loss: 0.99 | Train Accuracy: 0.53 |         Val. Loss: 1.09 | Val. Accuracy: 0.50 |\n",
            "Epoch took: 26.18484139442444\n",
            "| Epoch: 04 | Train Loss: 0.89 | Train Accuracy: 0.62 |         Val. Loss: 1.09 | Val. Accuracy: 0.44 |\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.45      0.53        33\n",
            "           1       0.20      0.08      0.11        13\n",
            "           2       0.46      0.85      0.60        20\n",
            "\n",
            "    accuracy                           0.50        66\n",
            "   macro avg       0.43      0.46      0.41        66\n",
            "weighted avg       0.49      0.50      0.47        66\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ponCV4YPQ0gl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}