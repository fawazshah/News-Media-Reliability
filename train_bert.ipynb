{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_bert.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO78tXSoCpV8X+2gXgzndZS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fawazshah/News-Media-Reliability/blob/master/train_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v3OZRfOXciF"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WdkCXGSlBCD",
        "outputId": "612ec53d-1a23-40a9-e4ce-2a687e847b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH89boyR8xMe"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import stanza\n",
        "import time\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5duyjiEYKPF"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD8oIXMaYJkJ"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "class HiddenPrints:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdQJh_Gspywv"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ltm1AzQ5P2p"
      },
      "source": [
        "corpus_url = 'https://raw.githubusercontent.com/fawazshah/News-Media-Reliability/master/data/emnlp18/corpus-modified.tsv'\n",
        "\n",
        "corpus = pd.read_csv(corpus_url, sep='\\t')\n",
        "urls = corpus['source_url_normalized'].values\n",
        "\n",
        "# Ground truths\n",
        "biases = corpus['bias'].values"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aljGD_19QdV"
      },
      "source": [
        "article_data_json_url = 'https://raw.githubusercontent.com/fawazshah/News-Media-Reliability/master/data/scraped_articles.json'\n",
        "\n",
        "r = requests.get(article_data_json_url)\n",
        "article_data = r.json()"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcqkrRXyw58K"
      },
      "source": [
        "all_data_df = pd.DataFrame(columns=['article headline', 'article body', 'bias'])\n",
        "\n",
        "news_sources_scraped = 0\n",
        "\n",
        "for row in corpus.itertuples():\n",
        "    url = row.source_url_normalized\n",
        "    bias = row.bias\n",
        "    if article_data[\"newspapers\"][url] is not None:\n",
        "        articles = article_data[\"newspapers\"][url].get(\"articles\", [])\n",
        "        if len(articles) > 0:\n",
        "            news_sources_scraped += 1\n",
        "            for article in articles:\n",
        "                all_data_df = all_data_df.append({'article headline': article['title'],\n",
        "                                                  'article body': article['text'],\n",
        "                                                  'bias': bias}, ignore_index=True)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "o8iAM1Mb1-Mr",
        "outputId": "b83bcb43-bf5c-445e-8b25-a80f0fa202ea"
      },
      "source": [
        "all_data_df"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article headline</th>\n",
              "      <th>article body</th>\n",
              "      <th>bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>On the Ground at the Inauguration: The Only Th...</td>\n",
              "      <td>Will Sennott\\n\\nWEDNESDAY, JANUARY 20, 2021, W...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Under President Biden, Will the Yankees Return...</td>\n",
              "      <td>Thurman Munson and Reggie Jackson in 1977 From...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Gun Rights Absolutists Celebrate Martin Luther...</td>\n",
              "      <td>Will Sennott\\n\\nMONDAY, JANUARY 18, 2021, RICH...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thugs in Blue</td>\n",
              "      <td>THE BEAT GOES ON … AND ON\\n\\nOnce Again, Polic...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HELL YEAH! Sheriff Clark Publicly DISEMBOWELS ...</td>\n",
              "      <td>Al Sharpton always has had a couple screws loo...</td>\n",
              "      <td>right</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1649</th>\n",
              "      <td>UK Educators Rank-and-File Safety Committee di...</td>\n",
              "      <td>The UK Educators Rank-and-File Safety Committe...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1650</th>\n",
              "      <td>Make It Sing</td>\n",
              "      <td>Before I lay into the Democrats for missed opp...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1651</th>\n",
              "      <td>Bill Maher: The SPIN Interview</td>\n",
              "      <td>If you care at all about democracy and the way...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>Stephan Jenkins on What Culture Truly Means</td>\n",
              "      <td>“When bad men combine, the good must associate...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1653</th>\n",
              "      <td>Emergence Is Interactive: A Jeff Bridges and S...</td>\n",
              "      <td>Jeff Bridges is an Academy Award-winning actor...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1654 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       article headline  ...   bias\n",
              "0     On the Ground at the Inauguration: The Only Th...  ...   left\n",
              "1     Under President Biden, Will the Yankees Return...  ...   left\n",
              "2     Gun Rights Absolutists Celebrate Martin Luther...  ...   left\n",
              "3                                         Thugs in Blue  ...   left\n",
              "4     HELL YEAH! Sheriff Clark Publicly DISEMBOWELS ...  ...  right\n",
              "...                                                 ...  ...    ...\n",
              "1649  UK Educators Rank-and-File Safety Committee di...  ...   left\n",
              "1650                                       Make It Sing  ...   left\n",
              "1651                     Bill Maher: The SPIN Interview  ...   left\n",
              "1652        Stephan Jenkins on What Culture Truly Means  ...   left\n",
              "1653  Emergence Is Interactive: A Jeff Bridges and S...  ...   left\n",
              "\n",
              "[1654 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3tI0VYup1RN"
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx8wTRfPI5jm",
        "outputId": "e489ce97-8b46-4e8a-f07e-aeb563a0b022"
      },
      "source": [
        "# Text preprocessing preparation\n",
        "\n",
        "stop_words = [\"the\", \"a\", \"an\", \"as\", \"this\", \"that\", \"is\", \"and\", \"or\", \"on\",\n",
        "              \"at\", \"to\", \"in\", \"by\", \"than\", \"of\", \"for\", \"be\", \"i\", \"you\", \n",
        "              \"he\", \"she\", \"his\", \"her\", \"do\", \"it\", \"with\"]\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# required for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# required for POS tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_vUJWRoo8uy"
      },
      "source": [
        "# Text preprocessing performed on both article headline and article body\n",
        "\n",
        "def preprocess(sentence):\n",
        "\n",
        "    # Lowercase\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Punctuation, whitespace removal\n",
        "    punctuations = '''!()-—[]{};:'\"“”‘’\\,<>./?@#$%^&*_~'''\n",
        "    whitespace = '''\\n\\t'''\n",
        "\n",
        "    for ch in sentence: \n",
        "        if ch in punctuations: \n",
        "            sentence = sentence.replace(ch, \"\")\n",
        "        if ch in whitespace:\n",
        "            sentence = sentence.replace(ch, \" \")\n",
        "\n",
        "    # Stop word removal\n",
        "    remaining_words = []\n",
        "    \n",
        "    for word in sentence.split():\n",
        "        if word not in stop_words:\n",
        "            remaining_words.append(word)\n",
        "\n",
        "    sentence = \" \".join(remaining_words)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_words = []\n",
        "\n",
        "    # In order to lemmatise we must first POS-tag each sentence\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "    for word, tag in tagged:\n",
        "        pos = nltk_tag_to_wordnet_tag(tag) \n",
        "        if pos is not None:\n",
        "            word = lemmatizer.lemmatize(word, pos=pos)\n",
        "\n",
        "        lemmatized_words.append(word)\n",
        "\n",
        "    sentence = \" \".join(lemmatized_words)\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3U6E9s6rRbxK",
        "outputId": "b6727e9b-64bd-4547-c0ea-a4b32cefc128"
      },
      "source": [
        "start = time.time()\n",
        "all_data_df['article headline'] = all_data_df['article headline'].apply(preprocess)\n",
        "print(f\"Preprocessing headlines took {time.time() - start} seconds\")\n",
        "\n",
        "start = time.time()\n",
        "all_data_df['article body'] = all_data_df['article body'].apply(preprocess)\n",
        "print(f\"Preprocessing article bodies took {time.time() - start} seconds\")"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing headlines took 1.429811716079712 seconds\n",
            "Preprocessing article bodies took 66.38098430633545 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ujhpSxfxckfm",
        "outputId": "e75057bb-a452-40df-af31-fc4a219afdec"
      },
      "source": [
        "all_data_df"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article headline</th>\n",
              "      <th>article body</th>\n",
              "      <th>bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ground inauguration only thing see be hope</td>\n",
              "      <td>will sennott wednesday january 20 2021 washing...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>under president biden will yankees return thei...</td>\n",
              "      <td>thurman munson reggie jackson 1977 from villag...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gun right absolutists celebrate martin luther ...</td>\n",
              "      <td>will sennott monday january 18 2021 richmond v...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>thug blue</td>\n",
              "      <td>beat go … once again police pummel plan reform...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hell yeah sheriff clark publicly disembowel ra...</td>\n",
              "      <td>al sharpton always have have couple screw loos...</td>\n",
              "      <td>right</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1649</th>\n",
              "      <td>uk educator rankandfile safety committee discu...</td>\n",
              "      <td>uk educator rankandfile safety committee meeti...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1650</th>\n",
              "      <td>make sing</td>\n",
              "      <td>before lay into democrat miss opportunity hous...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1651</th>\n",
              "      <td>bill maher spin interview</td>\n",
              "      <td>if care all about democracy way our world work...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>stephan jenkins what culture truly mean</td>\n",
              "      <td>when bad men combine good must associate else ...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1653</th>\n",
              "      <td>emergence interactive jeff bridge susan kucera...</td>\n",
              "      <td>jeff bridge academy awardwinning actor susan k...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1654 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       article headline  ...   bias\n",
              "0            ground inauguration only thing see be hope  ...   left\n",
              "1     under president biden will yankees return thei...  ...   left\n",
              "2     gun right absolutists celebrate martin luther ...  ...   left\n",
              "3                                             thug blue  ...   left\n",
              "4     hell yeah sheriff clark publicly disembowel ra...  ...  right\n",
              "...                                                 ...  ...    ...\n",
              "1649  uk educator rankandfile safety committee discu...  ...   left\n",
              "1650                                          make sing  ...   left\n",
              "1651                          bill maher spin interview  ...   left\n",
              "1652            stephan jenkins what culture truly mean  ...   left\n",
              "1653  emergence interactive jeff bridge susan kucera...  ...   left\n",
              "\n",
              "[1654 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCS4LqNqpw-0"
      },
      "source": [
        "### Splitting data into folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WVsMZJip6pD"
      },
      "source": [
        "# 5 folds, each with 70% training, 10% validation, 20% train\n",
        "\n",
        "num_folds = 5\n",
        "\n",
        "fold_size = round(len(all_data_df) / num_folds)\n",
        "fold_dfs = [all_data_df.iloc[i*fold_size:(i+1)*fold_size].copy() for i in range(num_folds)]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcLgT3JPrI5l"
      },
      "source": [
        "folds = {}\n",
        "\n",
        "for i, df in enumerate(fold_dfs):\n",
        "    folds[i] = {}\n",
        "    split_point_1 = int(0.7*len(df))\n",
        "    split_point_2 = int(0.8*len(df))\n",
        "    folds[i][\"train_df\"] = df.iloc[:split_point_1].copy()\n",
        "    folds[i][\"val_df\"] = df.iloc[split_point_1:split_point_2].copy()\n",
        "    folds[i][\"test_df\"] = df.iloc[split_point_2:].copy()"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv18NbamsoT8",
        "outputId": "0a97959a-90a9-4e05-9cca-c2c7c9c0e7d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"Number of folds: {num_folds}\")\n",
        "print(f\"Size of each training set: {len(folds[0]['train_df'])}\")\n",
        "print(f\"Size of each validation set: {len(folds[0]['val_df'])}\")\n",
        "print(f\"Size of each test set: {len(folds[0]['test_df'])}\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of folds: 5\n",
            "Size of each training set: 231\n",
            "Size of each validation set: 33\n",
            "Size of each test set: 67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRvLmb5gjsqW"
      },
      "source": [
        "### BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BALmXkmcDrdV"
      },
      "source": [
        "class BertDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noOaI7TwFfO2"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdMOKYheI2rI"
      },
      "source": [
        "# Compute the length of the longest sentence in particular column out of\n",
        "# all train, val and test data\n",
        "def compute_max_length(col_to_encode):\n",
        "\n",
        "  sentences = all_data_df[col_to_encode].values\n",
        "\n",
        "  max_len = 0\n",
        "\n",
        "  for sent in sentences:\n",
        "\n",
        "      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "      input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "      # Update the maximum sentence length.\n",
        "      max_len = max(max_len, len(input_ids))\n",
        "\n",
        "  return max_len"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLUvQg9tJkxL",
        "outputId": "47f172c7-8f35-4d20-df06-cde5626dba04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "max_len_headline = compute_max_length('article headline')\n",
        "max_len_body = compute_max_length('article body')\n",
        "\n",
        "print(f\"Max headline length across all folds: {max_len_headline}\")\n",
        "print(f\"Max article body length across all folds: {max_len_body}\")"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (910 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Max headline length across all folds: 103\n",
            "Max article body length across all folds: 14373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xefnAsZO59ZV"
      },
      "source": [
        "datasets = {}\n",
        "\n",
        "def create_bert_dataset(fold, df, col_to_encode, max_sequence_len):\n",
        "    # Returns a BertDataset of sequences in col_to_encode column of df, for\n",
        "    # given fold\n",
        "\n",
        "    sentences = df[col_to_encode].values.tolist()\n",
        "    encodings = tokenizer(sentences,\n",
        "                          add_special_tokens=True,\n",
        "                          max_length=max_sequence_len,\n",
        "                          padding='max_length',\n",
        "                          truncation=True,\n",
        "                          return_attention_mask = True,\n",
        "                          return_tensors = 'pt'\n",
        "                          )\n",
        "    return BertDataset(encodings, df['bias'].values)\n",
        "\n",
        "datasets['headlines'] = {}\n",
        "for i in range(num_folds):\n",
        "    datasets['headlines'][i] = {}\n",
        "    datasets['headlines'][i]['train'] = create_bert_dataset(i, folds[i]['train_df'],\n",
        "                                                            'article headline',\n",
        "                                                            max_len_headline)\n",
        "    datasets['headlines'][i]['val'] = create_bert_dataset(i, folds[i]['val_df'],\n",
        "                                                            'article headline',\n",
        "                                                            max_len_headline)\n",
        "    datasets['headlines'][i]['test'] = create_bert_dataset(i, folds[i]['test_df'],\n",
        "                                                            'article headline',\n",
        "                                                            max_len_headline)\n",
        "\n",
        "datasets['bodies'] = {}\n",
        "for i in range(num_folds):\n",
        "    datasets['bodies'][i] = {}\n",
        "    datasets['bodies'][i]['train'] = create_bert_dataset(i, folds[i]['train_df'],\n",
        "                                                            'article body',\n",
        "                                                            max_len_body)\n",
        "    datasets['bodies'][i]['val'] = create_bert_dataset(i, folds[i]['val_df'],\n",
        "                                                            'article body',\n",
        "                                                            max_len_body)\n",
        "    datasets['bodies'][i]['test'] = create_bert_dataset(i, folds[i]['test_df'],\n",
        "                                                            'article body',\n",
        "                                                            max_len_body)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4gDhKKtfvxt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}